# Weather Data Engineering Pipeline

> Automatiza la recolecciÃ³n, procesamiento y visualizaciÃ³n de datos climÃ¡ticos con Airflow, MinIO y Flask.

---

## ğŸ§  Â¿QuÃ© es este proyecto?

Este pipeline de datos meteorolÃ³gicos estÃ¡ diseÃ±ado para:

1. **Extraer** datos climÃ¡ticos por hora desde una API externa.
2. **Almacenar** los datos crudos en un data lake local (formato JSON).
3. **Transformar** los datos con `pandas` y guardarlos como Parquet.
4. **Visualizar** KPIs del clima en tiempo real mediante un dashboard web (Flask + Chart.js).

> Este proyecto forma parte de mi portafolio personal como prÃ¡ctica integral de data engineering.

---

## âš™ï¸ TecnologÃ­as utilizadas

- **Apache Airflow** â€“ OrquestaciÃ³n de pipelines
- **pandas** â€“ Procesamiento de datos
- **MinIO (S3 local)** â€“ Data lake local con compatibilidad S3
- **Flask** â€“ VisualizaciÃ³n web (dashboard)
- **Chart.js + HTML/CSS** â€“ KPIs y grÃ¡ficas de temperatura
- **Docker Compose** â€“ Contenedores para todos los servicios

---

## ğŸ—‚ï¸ Estructura del proyecto

weather_pipeline_project/
â”‚
â”œâ”€â”€ dags/ # DAGs de Airflow (extract + process)
â”œâ”€â”€ data_lake/ # Datos locales montados (crudos y procesados)
â”‚ â”œâ”€â”€ raw/
â”‚ â”œâ”€â”€ processed/
â”œâ”€â”€ minio_data/ # Datos almacenados por MinIO (weather-raw, weather-processed)
â”œâ”€â”€ flask_app/ # App web (Flask)
â”‚ â”œâ”€â”€ static/
â”‚ â”œâ”€â”€ templates/
â”‚ â””â”€â”€ app.py
â”œâ”€â”€ scripts/ # Scripts auxiliares (ej. eliminar buckets)
â”œâ”€â”€ notebooks/ # AnÃ¡lisis exploratorio (opcional, usando pandas)
â”œâ”€â”€ docker/ # ConfiguraciÃ³n adicional para Airflow
â”œâ”€â”€ docker-compose.yml # OrquestaciÃ³n con Docker Compose
â”œâ”€â”€ requirements.txt # Dependencias del proyecto
â””â”€â”€ README.md

---

## ğŸ“ˆ Dashboard final

El resultado es una aplicaciÃ³n web con:

- Temperatura actual, mÃ­nima, mÃ¡xima y promedio del dÃ­a.
- VisualizaciÃ³n por hora.
- GrÃ¡fica interactiva con evoluciÃ³n de la temperatura diaria.

![dashboard](dashboard/weather_dashboard.png)

---

## ğŸš€ CÃ³mo ejecutar este proyecto

### 1. Clonar el repositorio

---

- git clone https://github.com/tu_usuario/weather_pipeline_project.git
- cd weather_pipeline_project

### 2. Levantar la infraestructura

- docker-compose up --build

Esto levantarÃ¡:

- Apache Airflow (http://localhost:8080)
- MinIO (http://localhost:9001)

Correr el dashboard dentro de la carpeta flask_app (python app.py)

- Flask App (http://localhost:5000)

### 3. Activar los DAGs en Airflow

- Accede a http://localhost:8080

Credenciales por defecto:

Usuario: airflow
ContraseÃ±a: airflow

Activa los siguientes DAGs:

- extract_weather_data
- process_weather_data

Ambos correrÃ¡n automÃ¡ticamente cada hora.

### 4. Ver el dashboard

- Accede a http://localhost:5000


### ğŸ“Œ Extras y detalles

- Sensor inteligente: El DAG de procesamiento espera a que el de extracciÃ³n termine, gracias a un ExternalTaskSensor.

- Formateo limpio: Solo se almacena el Parquet del dÃ­a actual.


### ğŸ§  Aprendizajes

- CÃ³mo orquestar pipelines reales con Airflow y sensores dependientes.
- Uso de MinIO como alternativa local a S3.
- AutomatizaciÃ³n ETL real con datos externos.
- CreaciÃ³n de dashboards ligeros con Flask + Chart.js.
- impieza y transformaciÃ³n de datos con pandas.

### ğŸ‘¤ Autor

- [Alan Arturo Cano Sanchez](https://www.linkedin.com/in/alan-arturo-cano-sanchez-511855361)
ğŸš€ Â¡Gracias por visitar este proyecto!

---
